## üìë Tabla de Contenido

1. [üöÄ Instrucciones para Ejecutar el Proyecto](#-instrucciones-para-ejecutar-el-proyecto)  
   - [Requisitos Previos](#requisitos-previos)  
   - [Pasos para Ejecutar](#pasos-para-ejecutar)  
   - [Verificar el estado de los servicios](#verificar-el-estado-de-los-servicios)  
   - [Detener los servicios](#detener-los-servicios)

2. [üîó Probar las APIs](#-probar-las-apis)  
   - [1. Actualizar registros en Mongo desde el archivo cargado](#1-actualizar-registros-en-mongo-desde-el-archivo-cargado)  
   - [2. üåê Endpoints Disponibles en la aplicacion MELI API](#2-üåê-endpoints-disponibles-en-la-aplicacion-meli-api)  
     - [/items - Consulta de √çtems](#items---consulta-de-√≠tems)  
     - [/categories - Consulta de Categor√≠as](#categories---consulta-de-categor√≠as)  
     - [/currencies - Consulta de Monedas](#currencies---consulta-de-monedas)

3. [üìä Diagrama de Componentes](#-diagrama-de-componentes)

4. [üîÑ Diagrama de Estados](#-diagrama-de-estados)

5. [‚öôÔ∏è Arquitectura y Dise√±o](#Ô∏è-arquitectura-y-dise√±o)

6. [üìÅ Estructura del C√≥digo](#-estructura-del-c√≥digo)

7. [üìò Desaf√≠o Te√≥rico](#-desaf√≠o-te√≥rico)  
   - [1. Procesos, hilos y corrutinas](#1-procesos-hilos-y-corrutinas)  
   - [2. Optimizaci√≥n de recursos del sistema operativo](#2-optimizacion-de-recursos-del-sistema-operativo)  
   - [3. An√°lisis de complejidad](#3-an√°lisis-de-complejidad)



# üöÄ Instrucciones para Ejecutar el Proyecto

Este proyecto utiliza Docker para levantar dos APIs en Flask y una base de datos MongoDB junto con mongo-express, una interfaz web para explorar los datos de Mongo. Sigue estos pasos para correrlo desde cero:

## Requisitos Previos

- Tener instalado Docker y Docker Compose.

## Pasos para Ejecutar

1. **Clonar el repositorio**

```bash
git clone https://github.com/JassonM0lina/technical_challenge_meli
cd technical_challenge_meli
```

2. **Levantar los contenedores**

```bash
docker-compose  -f docker-compose.yaml up --build
```

Esto construir√° y levantar√° los siguientes servicios:

- üêç **meli_api** (puerto `5000`): API para consultar informaci√≥n de productos en Mercado Libre
- üß™ **integration_api** (puerto `5001`): API para actualizar la base de datos en MongoDB. 
- üçÉ **MongoDB** (puerto `27017`): Base de datos que almacena la informaci√≥n.
- üìä **mongo-express** (puerto `8081`): Interfaz web para visualizar los datos en Mongo.

> **Usuario/Contrase√±a para mongo-express:** `admin` / `admin`


3. **Verificar el estado de los servicios**
   Puedes usar el siguiente comando para ver los logs y verificar que todo est√° corriendo correctamente:

```bash
docker-compose logs -f
```

4. **Detener los servicios**

```bash
docker-compose  -f docker-compose.yaml down v 
```
## üîó Probar las APIs

Una vez los servicios est√©n corriendo, puedes probar las APIs con `curl` o Postman.


### 1. **Actualizar registros en Mongo desde el archivo  cargado**

```bash
POST: http://localhost:5001/update
```

Body JSON:
```json
{
  "name_file": "datalake.csv",
  "format": "csv",
  "separator": ",",
  "encoding": "utf-8"
}
```

#### üîß Par√°metros opcionales:

- `register_attributes`: lista de atributos que deseas guardar en Mongo. Si no se especifica, el valor por defecto es:
  ```json
  ["price", "name", "description"]
  ```
  - Si se agrega un atributo que no est√° soportado, se insertar√° como `null` en Mongo y se almacenar√° en la colecci√≥n `incomplete_register`.
  - Si se omite alguno de los atributos por defecto, el registro estar√° en `complete_register` pero sin ese atributo.
  - Los √∫nicos atributos que **siempre** estar√°n presentes son `id` y `site`.

- `len_batch`: n√∫mero de elementos que se procesar√°n por cada solicitud a la API de MELI. Valor por defecto: `50`.
  - Se ajust√≥ este par√°metro para evitar superar el l√≠mite de caracteres en peticiones HTTP (usualmente no m√°s de 2000 caracteres por GET).

---

### 2 üåê **Endpoints Disponibles en la aplicacion MELI API**

La aplicaci√≥n expone tres endpoints HTTP `GET` para consultar informaci√≥n directamente desde la API de Mercado Libre seg√∫n el tipo de entidad: √≠tems, categor√≠as o monedas.

### üîπ `/items` - Consulta de √çtems

Este endpoint permite consultar m√∫ltiples √≠tems de Mercado Libre junto con los atributos deseados. Este endpoint simula el API de mercado libre y la data se guarda como `meli_items_data.json`.

```bash
GET: http://localhost:5000/items
URL_EJEMPLO = /items?ids=MLA750925229,MLA845041373,MLA693105237&attributes=id,title,price
```

**Par√°metros:**
- `ids` (requerido): lista separada por comas de IDs de productos de Mercado Libre.
- `attributes` (opcional): lista separada por comas de atributos espec√≠ficos que deseas obtener por cada √≠tem (ej. `id`, `title`, `price`, `description`, etc.).

**Respuesta:**  
Un JSON con los datos solicitados para cada √≠tem.  


### üîπ `/categories` - Consulta de Categor√≠as

Este endpoint permite obtener informaci√≥n de una o varias categor√≠as de productos. Este endpoint simula el API de mercado libre y la data se guarda como `meli_categories_data.json`.

```bash
GET: http://localhost:5000/categories
URL_EJEMPLO = /categories?ids=MLA5725
```

**Par√°metros:**
- `ids` (requerido): ID de la categor√≠a de Mercado Libre.

**Respuesta:**  
Un JSON con los datos de la categor√≠a correspondiente.  

### üîπ `/currencies` - Consulta de Monedas

Este endpoint permite obtener informaci√≥n sobre las monedas que utiliza Mercado Libre (ej. ARS, BRL, COP, etc.). Este endpoint simula el API de mercado libre y la data se guarda como `meli_currencies_data.json`.

```bash
GET: http://localhost:5000/currencies
URL_EJEMPLO = /currencies?ids=ARS
```
**Par√°metros:**
- `ids` (requerido): ID de la moneda que deseas consultar.

**Respuesta:**  
Un JSON con los datos de la moneda especificada.  

---
### üîó Diagrama de Componentes

En la siguiente imagen podemos observar un **diagrama de componentes**, junto con los **puertos y URLs** a trav√©s de los cuales cada componente se comunica entre s√≠, incluyendo la interacci√≥n del usuario:

![Component Diagram](./assets/component_diagram.png)

---

### üîÑ Diagrama de Estados

En el siguiente diagrama se puede observar el **diagrama de estados** que el programa sigue para la separaci√≥n de responsabilidades. A continuaci√≥n, se describe el flujo del sistema:

1. **Estado 1 - `State_Init`**: Se encarga de leer el archivo de texto fila por fila (**A**) y, cuando se acumula un batch de `x` filas (normalmente 50), se env√≠a dicho batch al estado 2, **`State_Cache`** (**B**).
2. **Estado 2 - `State_Cache`**: Revisa cu√°les de las `x` filas ya est√°n almacenadas en la base de datos usando consultas en modo bulk. Las que a√∫n no est√°n registradas en Mongo se env√≠an al estado **`State_Info`** (**C**).
3. **Estado 3 - `State_Info`**: Completa la informaci√≥n de cada fila haciendo un request al API de Mercado Libre (**G**).  
   - Si la informaci√≥n est√° completa, el registro se guarda en la colecci√≥n **`complete_register`**.  
   - Si falta alg√∫n dato o hay errores de respuesta, el registro se guarda en **`incomplete_register`** (**F**).  
   - Luego de procesar todo el batch, retorna al estado inicial **`State_Init`** para continuar con el siguiente grupo de filas (**D**) hasta completar todo el archivo (**A**).

![State Diagram](./assets/state_diagram.png)

---

### ‚öôÔ∏è Arquitectura y Dise√±o

El c√≥digo del desaf√≠o fue implementado 100% con **Programaci√≥n Orientada a Objetos (POO)** y, para modelar el flujo anterior, se utiliz√≥ el **patr√≥n de dise√±o de estado**.

Adem√°s, se siguieron los principios **SOLID**, se aplic√≥ una **arquitectura hexagonal**, **Domain-Driven Design (DDD)** y **vertical slicing**, con el fin de garantizar **flexibilidad, extensibilidad y mantenibilidad** del sistema.

Cada _feature_ posee su propia estructura con carpetas `application`, `domain` e `infrastructure`. Esto permite, por ejemplo, cambiar la base de datos o el endpoint del API de Mercado Libre sin modificar la l√≥gica de negocio, ya que el **caso de uso est√° desacoplado de los recursos externos**.

---

### üìÅ Estructura del C√≥digo

- La implementaci√≥n de los tres estados `State_Init`, `State_Cache` y `State_Info`, que corresponden a los casos de uso de negocio, se encuentra en:  
  `./integration_api/fetch_integrate/application/pipeline_state.py`

- La conexi√≥n a los recursos externos (`open_file`, `database`, `request`) est√° ubicada en:  
  `./integration_api/fetch_integrate/infraestructure/integration_connection.py`

- La carpeta de dominio contiene constantes, par√°metros de entrada, interfaces abstractas, el archivo de texto y el contexto de los estados, que act√∫a como patr√≥n mediador.

- Adicionalmente, se implement√≥ el patr√≥n *Facade*, que funciona como la interfaz mediante la cual el usuario puede realizar diferentes consultas HTTP.  
  Es importante mencionar que tanto `app.py` como `main.py` hacen uso de esta interfaz *Facade*. Sin embargo, `app.py` corresponde a la aplicaci√≥n Flask. Esto significa que si se desea cambiar a FastAPI, la modificaci√≥n es sencilla, ya que no afecta el caso de uso.

- Por √∫ltimo, se incluye un archivo adicional `docker-compose.debug.yaml` que ejecuta √∫nicamente la aplicaci√≥n `meli_app`, `mongo` y `mongo_express`.  
  En este entorno, es posible correr `main.py` directamente para ejecutar el programa de forma similar a como lo har√≠a Flask. Incluso si se est√° utilizando el `docker-compose.yaml` principal, se puede seguir ejecutando `main.py` con la opci√≥n de *debug* de VSCode.  
  Esto facilita considerablemente el desarrollo, ya que evita la necesidad de subir y bajar constantemente el contenedor `integration_api`, o mantenerlo activo en segundo plano, lo que consumir√≠a m√°s recursos de RAM del equipo.

---
# üìò Desaf√≠o Te√≥rico

## 1. Procesos, hilos y corrutinas

### ‚óè Un caso en el que usar√≠as procesos para resolver un problema y por qu√©

Usar√≠a procesos cuando es necesaria la ejecuci√≥n de tareas que consumen muchos recursos de CPU pero que pueden ejecutarse en paralelo, como el procesamiento de im√°genes o los c√°lculos matem√°ticos pesados. La principal ventaja es que cada proceso tiene su propio espacio de memoria, lo cual permite evitar conflictos, aunque tambi√©n hace m√°s dif√≠cil compartir datos entre ellos. Aun as√≠, permite aprovechar m√∫ltiples n√∫cleos del procesador. En consecuencia, si un proceso falla, no afecta directamente a los dem√°s, lo que proporciona mayor robustez en tareas intensivas.

### ‚óè Un caso en el que usar√≠as threads para resolver un problema y por qu√©

Usar√≠a hilos principalmente cuando estoy trabajando con operaciones de entrada/salida o tareas que requieren esperar por recursos externos, como llamadas a APIs, consultas a BBDD, manejar conexiones de red o lectura de archivos. A diferencia de los procesos, los hilos son m√°s ligeros y comparten el mismo espacio de memoria, lo que permite una comunicaci√≥n r√°pida entre ellos.

### ‚óè Un caso en el que usar√≠as corrutinas para resolver un problema y por qu√©

Usar√≠a corrutinas cuando necesito manejar muchas tareas concurrentes que dependen de operaciones as√≠ncronas, como hacer miles de solicitudes a una API, interactuar con bases de datos en tiempo real o leer datos de manera secuencial sin bloquear el sistema. Las corrutinas permiten escribir c√≥digo as√≠ncrono de forma m√°s simple, consumen menos memoria que los hilos tradicionales y son ideales para aplicaciones donde la eficiencia y la escalabilidad en operaciones de entrada/salida (I/O) son importantes.

En el lenguaje de programaci√≥n Python, solo es posible ejecutar un hilo a la vez debido al GIL (Global Interpreter Lock), una restricci√≥n del int√©rprete CPython que evita la ejecuci√≥n simult√°nea de m√∫ltiples hilos en tareas CPU-bound. Esto se debe a c√≥mo Python maneja internamente la memoria y los objetos. Por esta raz√≥n, cuando se crean m√∫ltiples hilos, Python les asigna peque√±os fragmentos de tiempo de ejecuci√≥n de manera secuencial, dando la impresi√≥n de que se ejecutan en paralelo. Si se requiere una ejecuci√≥n real en paralelo es necesario utilizar el m√≥dulo multiprocessing, que crea procesos independientes con su propia memoria. Por otro lado, las corrutinas (async/await) en Python funcionan sobre un solo hilo, pero permiten continuar la ejecuci√≥n mientras se espera por una operaci√≥n I/O (como una consulta a una base de datos o una solicitud HTTP). Cuando una tarea necesita esperar, el control se cede a otra corrutina disponible, lo que mejora la eficiencia sin necesidad de m√∫ltiples hilos o procesos.

El multithreading en Python suele utilizarse para tareas I/O. Sin embargo, tambi√©n resulta √∫til en aplicaciones de rob√≥tica. Por ejemplo, lo he usado en peque√±os robots seguidores de l√≠nea con microprocesadores de un solo n√∫cleo y bater√≠as limitadas. En estos casos, Python permite crear m√∫ltiples hilos que ejecutan tareas breves, como la lectura de distintos sensores, de forma intercalada. Aunque solo un hilo se ejecuta a la vez, el cambio r√°pido entre ellos da la sensaci√≥n de que todos los sensores se est√°n leyendo simult√°neamente.

## 2. Optimizaci√≥n de recursos del sistema operativo

### ‚óè Si tuvieras 1.000.000 de elementos y tuvieras que consultar para cada uno de ellos informaci√≥n en una API HTTP. ¬øC√≥mo lo har√≠as? Explicar.

Lo har√≠a de forma concurrente y controlada, optimizando el uso de recursos y evitando sobrecargar la API. Para lograr esto, aplicar√≠a las siguientes estrategias:

- Usar√≠a corrutinas como async/await en Python para lanzar m√∫ltiples solicitudes HTTP de manera concurrente, aprovechando que las llamadas a APIs son operaciones de entrada/salida.
- Para evitar errores por exceso de peticiones, como el ‚Äú429 Too Many Requests‚Äù, implementar√≠a un sistema de rate limiting para limitar la cantidad de solicitudes simult√°neas y respetar los l√≠mites de la API.
- Si la API permite hacer peticiones por lotes (batch requests), agrupar√≠a varios elementos en una misma solicitud para reducir el n√∫mero total de llamadas.
- Incluir√≠a una l√≥gica de reintentos con backoff exponencial para gestionar fallos temporales o errores de red.
- Dada la cantidad de peticiones, implementar√≠a un sistema de colas como Kafka para lograr una mejor persistencia y recuperaci√≥n controlada. Adem√°s, almacenar√≠a las peticiones que no fueron respondidas exitosamente (despu√©s de cierto n√∫mero de reintentos con backoff exponencial) en Kafka o en una base de datos para reprocesarlas luego.
- Si el procesamiento de la respuesta de cada elemento se puede realizar de forma independiente, considerar√≠a el uso de multiprocesamiento para aprovechar m√∫ltiples n√∫cleos del procesador y distribuir la carga.
- Implementar√≠a un sistema de monitoreo para registrar m√©tricas clave como el tiempo de respuesta de la API, n√∫mero de reintentos, tasa de √©xito y de fallos. Esto permite detectar cuellos de botella o comportamientos an√≥malos inesperados. Se puede implementar generando logs estructurados o utilizando herramientas como Prometheus o Grafana.
- Implementar√≠a una capa de cach√© para evitar llamadas innecesarias a la API en casos de elementos o respuestas repetitivas, reduciendo as√≠ la carga total del sistema.
- Podr√≠a distribuir la carga dividiendo el proceso en m√∫ltiples nodos o servicios distribuidos, utilizando herramientas como Kubernetes para lograr escalabilidad horizontal y resiliencia.

## 3. An√°lisis de complejidad

### ‚óè Dados 4 algoritmos A, B, C y D que cumplen la misma funcionalidad, con complejidades O(n¬≤), O(n¬≥), O(2‚Åø) y O(n log n), respectivamente, ¬øCu√°l de los algoritmos favorecer√≠as y cu√°l descartar√≠as en principio? Explicar por qu√©.

Favorecer√≠a el algoritmo O(n log n) y descartar√≠a el algoritmo O(2‚Åø), dado que la complejidad algor√≠tmica nos da una estimaci√≥n del crecimiento del tiempo de ejecuci√≥n en funci√≥n del tama√±o de los datos de entrada (n).

- O(n¬≤) y O(n¬≥) pueden ser aceptables para entradas peque√±as, e incluso medianas, pero hay que tener en cuenta que su rendimiento se degrada con rapidez a medida que n crece, especialmente en el caso de la complejidad c√∫bica. El algoritmo burbuja tiene una complejidad en tiempo de O(n¬≤), y la multiplicaci√≥n de matrices con triple bucle tiene una complejidad en tiempo de O(n¬≥).
- O(2‚Åø) representa un crecimiento exponencial, lo que lo hace muy ineficiente incluso para valores peque√±os de n. Este tipo de algoritmo deber√≠a evitarse y reemplazarse por otro m√°s eficiente y optimizado. El algoritmo de Fibonacci recursivo sin memorizaci√≥n tiene esta complejidad en tiempo.
- O(n log n) es la complejidad m√°s eficiente entre las opciones dadas y es ideal para grandes vol√∫menes de datos, ya que escala de forma razonable. Algoritmos como Merge Sort o Heap Sort tienen esta complejidad en tiempo.

### ‚óè Asume que dispones de dos bases de datos para utilizar en diferentes problemas a resolver. La primera, llamada AlfaDB, tiene una complejidad de O(1) en consulta y O(n¬≤) en escritura. La segunda, llamada BetaDB, tiene una complejidad de O(log n) tanto para consulta como para escritura. Describe, en forma sucinta, qu√© casos de uso podr√≠as atacar con cada una.

- **AlfaDB**: usar√≠a esta base de datos en casos donde se realizan consultas muy frecuentes y escrituras poco frecuentes, dado que ofrece consultas en tiempo constante O(1). Esto la hace ideal para aplicaciones donde la velocidad de lectura es prioritaria y se puede tolerar un alto costo en las escrituras, como en sistemas de cach√© o bases de datos que almacenan configuraciones cr√≠ticas que rara vez cambian pero se consultan con frecuencia, por ejemplo, para reiniciar sistemas.

- **BetaDB**: la usar√≠a en situaciones donde se requiere un rendimiento equilibrado entre operaciones de lectura y escritura, ya que ambas tienen una complejidad de O(log n). Esto la hace adecuada para sistemas transaccionales (OLTP), plataformas con usuarios activos o aplicaciones en tiempo real que necesitan consistencia y eficiencia en ambas operaciones.